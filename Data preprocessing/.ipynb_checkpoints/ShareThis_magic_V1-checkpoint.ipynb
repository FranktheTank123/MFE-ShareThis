{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Data processing part by Frank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## work in pyspark\n",
    "import json\n",
    "import os\n",
    "import pyspark\n",
    "from datetime import date , datetime\n",
    "from pyspark.sql.types import StringType, TimestampType, IntegerType\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "day = '20150217'\n",
    "num_of_domain = 100\n",
    "region = \"us\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set path\n",
    "file_path =  \"s3n://log.sharethis.com/amankesarwani/\" + region + \"/\" + str(day) + \"/part-000000000001*\"\n",
    "stock_return_path = \"s3n://log.sharethis.com/Stock_Proceesed_return_Frank_lag.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DON'T TOUCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IT MAGIC\n",
    "\n",
    "let's clean the stock data\n",
    "\"\"\"\n",
    "stock_return_raw = spark.read.csv(stock_return_path, sep=\",\", header=True)\n",
    "## clean the time\n",
    "stock_return_cleaned = stock_return_raw.withColumn(\"TimeStamp\", from_utc_timestamp(stock_return_raw.Lagged_Time, \"UTC\")) \\\n",
    "                        .drop(\"Lagged_Time\").withColumnRenamed(\"variable\", \"Ticker\")\n",
    "\n",
    "stock_return_cleaned.cache()\n",
    "\n",
    "SP500_tickers = [x.Ticker for x in stock_return_cleaned.select(\"Ticker\").distinct().collect()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "503"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SP500_tickers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THIS IS ALSO MAGIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Then we process the sentiment data\n",
    "\"\"\"\n",
    "## personalized UDF\n",
    "ricToTicker = udf(lambda x: x.split('.')[0])\n",
    "\n",
    "# load data\n",
    "sharethis_json = spark.read.json(file_path) ## spark 2.0\n",
    "\n",
    "# data process\n",
    "temp_json_raw = sharethis_json.select('*',explode(col('companies')).alias('tempCol')).drop('companies')\n",
    "json_cleaned = (temp_json_raw.select( '*' ,temp_json_raw.tempCol.getItem(\"count\").alias('company_count'),\n",
    "    temp_json_raw.tempCol.getItem(\"sentiment_score\").alias('company_sentiment_score'),\n",
    "    temp_json_raw.tempCol.getItem(\"ric\").alias('company_ric'))\n",
    "    .drop('tempCol').drop('stid').drop('url').drop('userAgent')\n",
    "    .filter(col('company_ric').isNotNull())\n",
    "    .filter(col('company_sentiment_score').isNotNull())\n",
    "    .withColumn(\"Ticker\", ricToTicker(col(\"company_ric\"))).drop(\"company_ric\")\n",
    "    )\n",
    "\n",
    "## cache the data\n",
    "json_cleaned.cache()\n",
    "\n",
    "## we only need SP500 tickers\n",
    "json_cleaned = json_cleaned.filter(col(\"Ticker\").isin(SP500_tickers))\n",
    "\n",
    "\n",
    "device_categories = [u'Personal computer', u'Tablet', u'Smartphone']\n",
    "\n",
    "refDomain_categories = (json_cleaned.groupBy(\"refDomain\")\n",
    "                        .count().orderBy(desc(\"count\")).select(\"refDomain\")\n",
    "                        .rdd.flatMap(lambda x: x).top(num_of_domain))\n",
    "\n",
    "refDomain_categories_filter = []\n",
    "for x in refDomain_categories:\n",
    "    try:\n",
    "        temp = x.split('.')[-2]\n",
    "        if temp not in refDomain_categories_filter:\n",
    "            refDomain_categories_filter.append(temp)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "exprs_device = [F.when(F.col(\"deviceType\") == category, 1).otherwise(0).alias(\"is_device_\"+category)\n",
    "        for category in device_categories]\n",
    "exprs_domain = [F.when(F.col(\"refDomain\") == category, 1).otherwise(0).alias(\"is_domain_\"+category)\n",
    "        for category in refDomain_categories_filter]\n",
    "\n",
    "labeled_json_cleaned = json_cleaned.select(\"*\", *exprs_device) \\\n",
    "        .select(\"*\", *exprs_domain).drop(\"deviceType\").drop(\"refDomain\")\n",
    "\n",
    "## finally we parse the time\n",
    "parse_sharethis_time = udf(lambda x: datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"), TimestampType())\n",
    "getHours = udf(lambda x: x.hour , IntegerType())\n",
    "labeled_json_final = labeled_json_cleaned.withColumn(\"TimeStamp\", parse_sharethis_time(col(\"standardTimestamp\"))).drop(\"standardTimestamp\")\n",
    "\n",
    "#labeled_json_final = json_cleaned.withColumn(\"TimeStamp\", parse_sharethis_time(col(\"standardTimestamp\"))).drop(\"standardTimestamp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "THIS IS TRUE MAGIC\n",
    "\"\"\"\n",
    "joined_dataframe = labeled_json_final.join(stock_return_cleaned, [\"TimeStamp\", \"Ticker\"], how = \"left_outer\")\n",
    "\n",
    "joined_dataframe_filtered = joined_dataframe.filter(col('Return').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[TimeStamp: timestamp, Ticker: string, browserFamily: string, channel: string, mappedEvent: string, os: string, shortIp: string, company_count: string, company_sentiment_score: string, is_device_Personal computer: int, is_device_Tablet: int, is_device_Smartphone: int, is_domain_zoomfreegames: int, is_domain_zooatlanta: int, is_domain_zonable: int, is_domain_zeeklytv: int, is_domain_zamm: int, is_domain_zackhunt: int, is_domain_youtump3: int, is_domain_yonanas: int, is_domain_yandex: int, is_domain_yahoo: int, is_domain_xhamster-proxy: int, is_domain_wavy: int, is_domain_teen: int, is_domain_smartbrief: int, is_domain_alcatel-lucent: int, is_domain_zyrtecprofessional: int, is_domain_zoho: int, is_domain_ziggityzoom: int, is_domain_zetronix: int, is_domain_zergnet: int, is_domain_zapmeta: int, is_domain_zabasearch: int, is_domain_yummymummyclub: int, is_domain_yugatech: int, is_domain_ytvids: int, is_domain_yr: int, is_domain_youtubeproxy: int, is_domain_yoursouthernforddealers: int, is_domain_yourmidwestforddealers: int, is_domain_younghollywood: int, is_domain_youngcons: int, is_domain_youmail: int, is_domain_yoshimura-rd: int, is_domain_yidio: int, is_domain_yhc: int, is_domain_yellowjacketcase: int, is_domain_xpbargains: int, is_domain_xoriant: int, is_domain_xnxxporn: int, is_domain_wxyz: int, is_domain_wxii12: int, is_domain_wvondemand: int, is_domain_wvgazette: int, is_domain_wtmlondon: int, is_domain_wsoctv: int, is_domain_wscc: int, is_domain_wsbtv: int, is_domain_wsbradio: int, is_domain_writingexcuses: int, is_domain_writersdigest: int, is_domain_wrestlinggear: int, is_domain_wpxi: int, is_domain_wptv: int, is_domain_worldofcoca-cola: int, is_domain_worksheetsworksheets: int, is_domain_workinsports: int, is_domain_workingsolutionsjobs: int, is_domain_wordstream: int, is_domain_woodcraft: int, is_domain_wonderwall: int, is_domain_wnd: int, is_domain_wmur: int, is_domain_wkbw: int, is_domain_wisesales: int, is_domain_wisegeekhealth: int, is_domain_wisebread: int, is_domain_winnipegfreepress: int, is_domain_wini: int, is_domain_wingeyecare: int, is_domain_winereviewonline: int, is_domain_wineinsiders: int, is_domain_windowssearch: int, is_domain_windows8downloads: int, is_domain_windows7download: int, is_domain_williamblair: int, is_domain_widerun: int, is_domain_wholefoodsmarket: int, is_domain_whitakercenter: int, is_domain_whio: int, is_domain_whereincity: int, is_domain_whattogiveupforlent: int, is_domain_wfrb: int, is_domain_wetmummy: int, is_domain_westernplows: int, is_domain_westernjournalism: int, is_domain_wesh: int, is_domain_weny: int, is_domain_wellbeingalignment: int, is_domain_weiunderpar: int, is_domain_webcrawler: int, is_domain_wdwinfo: int, is_domain_wcupa: int, is_domain_wcpo: int, is_domain_wbintv: int, is_domain_mn: int, is_domain_ca: int, is_domain_washingtontimes: int, is_domain_washingtonpost: int, is_domain_ford: int, Return: string]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_dataframe_filtered.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|company_count|\n",
      "+-------------+\n",
      "|            3|\n",
      "|            2|\n",
      "|            3|\n",
      "|            1|\n",
      "|            1|\n",
      "|            2|\n",
      "|            1|\n",
      "|            3|\n",
      "|            5|\n",
      "|            1|\n",
      "|           15|\n",
      "|            2|\n",
      "|            1|\n",
      "|            1|\n",
      "|            1|\n",
      "|            1|\n",
      "|            1|\n",
      "|            1|\n",
      "|           15|\n",
      "|            1|\n",
      "|            2|\n",
      "|            2|\n",
      "|            3|\n",
      "|            1|\n",
      "|            4|\n",
      "|            1|\n",
      "|            1|\n",
      "|            5|\n",
      "|            7|\n",
      "|            2|\n",
      "|            1|\n",
      "|            9|\n",
      "|            2|\n",
      "|            2|\n",
      "|            1|\n",
      "|            5|\n",
      "|            1|\n",
      "|            4|\n",
      "|            1|\n",
      "|            1|\n",
      "|            3|\n",
      "|            1|\n",
      "|            1|\n",
      "|            1|\n",
      "|            1|\n",
      "|            1|\n",
      "|            1|\n",
      "|            1|\n",
      "|            4|\n",
      "|            1|\n",
      "+-------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_dataframe_filtered.select(\"company_count\").show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First we creat T/F labels for return \n",
    "joined_dataframe_filtered = joined_dataframe_filtered.withColumn(\"Label\",joined_dataframe_filtered[\"Return\"]>0)\n",
    "#NOTE THAT WE NEED TO USE A STRING INSTANCE WHEN USING CAST\n",
    "joined_dataframe_filtered = joined_dataframe_filtered.withColumn(\"Label\",joined_dataframe_filtered[\"Label\"].cast(\"Double\"))\n",
    "#joined_dataframe_filtered.select(\"Label\").show(50)     #become 0/1 based\n",
    "#now look at if votes are balanced\n",
    "joined_dataframe_filtered.filter(joined_dataframe_filtered.Label==1).count()#clearly not balanced...\n",
    "#437T:17F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Machine Learning ðŸ˜¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import BooleanType,DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#let's look at what cols we have here...\n",
    "col=joined_dataframe_filtered.columns\n",
    "#dropping redundant columns, need a better way...\n",
    "col.remove(\"Return\")\n",
    "col.remove(\"TimeStamp\")\n",
    "col.remove(\"Ticker\")\n",
    "col.remove(\"channel\")\n",
    "col.remove(\"browserFamily\")\n",
    "col.remove(\"mappedEvent\")\n",
    "col.remove(\"os\")\n",
    "col.remove(\"shortIp\")\n",
    "#col.remove(\"company_count\")\n",
    "#col.remove(\"company_sentiment_score\")\n",
    "#convert sentiment score into double\n",
    "\n",
    "joined_dataframe_filtered = joined_dataframe_filtered.withColumn(\"company_sentiment_score\",\n",
    "                                                                 joined_dataframe_filtered\n",
    "                                                                 [\"company_sentiment_score\"].cast(\"Double\"))\n",
    "\n",
    "\n",
    "#cast company_count into int\n",
    "joined_dataframe_filtered = joined_dataframe_filtered.withColumn(\"company_count\",\n",
    "                                                                 joined_dataframe_filtered\n",
    "                                                                 [\"company_count\"].cast(\"Integer\"))\n",
    "\n",
    "#joined_dataframe_filtered.printSchema()\n",
    "#joined_dataframe_filtered.select(\"company_sentiment_score\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorAssembler_470b8209af64067ea556"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = VectorAssembler()\n",
    "vectorizer.setInputCols(col)\n",
    "vectorizer.setOutputCol(\"Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 1014\n",
    "(split20DF, split80DF) = joined_dataframe_filtered.randomSplit([0.2,0.8],seed)\n",
    "\n",
    "# Let's cache these datasets for performance\n",
    "testSet = split20DF\n",
    "trainingSet = split80DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlossType: Loss function which GBT tries to minimize (case-insensitive). Supported options: logistic (default: logistic)\\nmaxBins: Max number of bins for discretizing continuous features.  \\nMust be >=2 and >= number of categories for any categorical feature. (default: 32)\\nmaxDepth: Maximum depth of the tree. (>= 0) \\nE.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\\nmaxIter: max number of iterations (>= 0). (default: 20)\\nminInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\\nminInstancesPerNode: Minimum number of instances each child must have after split.\\nIf a split causes the left or right child to have fewer than minInstancesPerNode, \\nthe split will be discarded as invalid. Should be >= 1. (default: 1)\\nstepSize: Step size to be used for each iteration of optimization (>= 0). (default: 0.1)\\nsubsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (undefined)\\n'"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt=GBTClassifier()\n",
    "#print(gbt.explainParams())\n",
    "\"\"\"\n",
    "lossType: Loss function which GBT tries to minimize (case-insensitive). Supported options: logistic (default: logistic)\n",
    "maxBins: Max number of bins for discretizing continuous features.  \n",
    "Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
    "maxDepth: Maximum depth of the tree. (>= 0) \n",
    "E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\n",
    "maxIter: max number of iterations (>= 0). (default: 20)\n",
    "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
    "minInstancesPerNode: Minimum number of instances each child must have after split.\n",
    "If a split causes the left or right child to have fewer than minInstancesPerNode, \n",
    "the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
    "stepSize: Step size to be used for each iteration of optimization (>= 0). (default: 0.1)\n",
    "subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (undefined)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GBTClassifier_49c497ea12b0f11262b4"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt.setLabelCol(\"Label\") \\\n",
    "    .setPredictionCol(\"Predicted_Label\") \\\n",
    "    .setFeaturesCol(\"Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a pipeline\n",
    "gbtPipeline=Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline_477f9ebec914a91db854"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the stages of the Pipeline\n",
    "gbtPipeline.setStages([vectorizer,gbt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#first glimps\n",
    "#NOTE THAT PIPELINE/VECTORIZER FIT CURRENTLY ONLY SUPPORT NUMERIC FEATURES??????\n",
    "gbt_glimps1=gbtPipeline.fit(trainingSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make prediction\n",
    "test_predictions = gbt_glimps1.transform(testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_label_test=test_predictions.select(\"Predicted_Label\", \"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = BinaryClassificationEvaluator(labelCol= \"Label\", rawPredictionCol=\"Predicted_Label\",metricName=\"areaUnderROC\")\n",
    "accuracy = evaluator.evaluate(predicted_label_test)\n",
    "accuracy\n",
    "#ðŸ˜¯, this is wierd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|Predicted_Label|Label|\n",
      "+---------------+-----+\n",
      "|            0.0|  0.0|\n",
      "|            0.0|  0.0|\n",
      "|            0.0|  0.0|\n",
      "|            1.0|  1.0|\n",
      "|            0.0|  0.0|\n",
      "|            0.0|  0.0|\n",
      "|            0.0|  0.0|\n",
      "|            0.0|  0.0|\n",
      "|            0.0|  0.0|\n",
      "|            0.0|  0.0|\n",
      "+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_label_test.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#explore feature importanace\n",
    "#...and there is no feature importance for GBT as for now\n",
    "#available for random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#explore cross validation\n",
    "# Let's create our CrossValidator with 3 fold cross validation\n",
    "crossval = CrossValidator(estimator=gbtPipeline, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "# Let's tune over our regularization parameter subsamplingRate from 0 to 1\n",
    "subsamplingRate = [x / 10.0 for x in range(1, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossValidator_45da9dc9281a9ab0a9ef"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll create a paramter grid using the ParamGridBuilder, and add the grid to the CrossValidator\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbt.subsamplingRate, subsamplingRate)\n",
    "             .build())\n",
    "crossval.setEstimatorParamMaps(paramGrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now let's find and return the best model\n",
    "cvModel = crossval.fit(trainingSet).bestModel\n",
    "\n",
    "#predict on test set\n",
    "predicted_label_test_cv=cvModel.transform(testSet).select(\"Predicted_Label\", \"Label\")\n",
    "\n",
    "#evaluate the accuracy by AUC\n",
    "evaluator = BinaryClassificationEvaluator(labelCol= \"Label\", rawPredictionCol=\"Predicted_Label\",metricName=\"areaUnderROC\")\n",
    "accuracy = evaluator.evaluate(predicted_label_test_cv)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier_4ee6992e918f85de6087"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = RandomForestClassifier()\n",
    "\n",
    "dt.setLabelCol(\"Label\")\\\n",
    "  .setPredictionCol(\"Predicted_Label\")\\\n",
    "  .setFeaturesCol(\"Features\")\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtPipeline=Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline_4308ab36ea4a8a907f94"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtPipeline.setStages([vectorizer,dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. (default: 10)\n",
      "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2 (0.0-1.0], [1-n]. (default: auto)\n",
      "featuresCol: features column name. (default: features, current: Features)\n",
      "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini (default: gini)\n",
      "labelCol: label column name. (default: label, current: Label)\n",
      "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\n",
      "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
      "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
      "numTrees: Number of trees to train (>= 1). (default: 20)\n",
      "predictionCol: prediction column name. (default: prediction, current: Predicted_Label)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "seed: random seed. (default: -4140900678877021401)\n",
      "subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(dt.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#first glimps\n",
    "#NOTE THAT PIPELINE/VECTORIZER FIT CURRENTLY ONLY SUPPORT NUMERIC FEATURES??????\n",
    "dt_glimps1=dtPipeline.fit(trainingSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make prediction\n",
    "test_predictions = dt_glimps1.transform(testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_label_test=test_predictions.select(\"Predicted_Label\", \"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = BinaryClassificationEvaluator(labelCol= \"Label\", rawPredictionCol=\"Predicted_Label\",metricName=\"areaUnderROC\")\n",
    "accuracy = evaluator.evaluate(predicted_label_test)\n",
    "accuracy\n",
    "#ðŸ˜¯, this is wierd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#look at feature importance\n",
    "rf_model1=dt_glimps1.stages[1]\n",
    "\n",
    "rf_model1.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#try CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#explore cross validation\n",
    "# Let's create our CrossValidator with 3 fold cross validation\n",
    "crossval = CrossValidator(estimator=dtPipeline, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "# Let's tune over our regularization parameter featureSubsetStrategy\n",
    "featureSubsetStrategy = [x for x in [\"sqrt\",\"auto\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossValidator_41db9d871e14b3f3ef4b"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll create a paramter grid using the ParamGridBuilder, and add the grid to the CrossValidator\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(dt.featureSubsetStrategy,featureSubsetStrategy )\n",
    "             .build())\n",
    "crossval.setEstimatorParamMaps(paramGrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's find and return the best model\n",
    "cvModel_rf = crossval.fit(trainingSet).bestModel\n",
    "\n",
    "#predict on test set\n",
    "predicted_label_test_cv_rf=cvModel.transform(testSet).select(\"Predicted_Label\", \"Label\")\n",
    "\n",
    "#evaluate the accuracy by AUC\n",
    "evaluator = BinaryClassificationEvaluator(labelCol= \"Label\", rawPredictionCol=\"Predicted_Label\",metricName=\"areaUnderROC\")\n",
    "accuracy = evaluator.evaluate(predicted_label_test_cv_rf)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
