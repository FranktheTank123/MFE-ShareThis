ceData$pos_conf <- as.numeric(ceData$pos_conf)
ceData$neg_uni <- as.numeric(ceData$neg_uni)
ceData$pos_uni <- as.numeric(ceData$pos_uni)
ceData$tot_uni <- as.numeric(ceData$tot_uni)
ceData
}
iqnData <- loadIQN.load()
View(iqnData, title = "IQN")
source('~/FranktheTank_git/inferess-haas-mfe/iqn/load-iqn.R', echo=TRUE)
str(iqnData)
?read.delim
# Load IQN for WQ
# loadIQN.convertDate <- function(dateString) {
#   as.POSIXct(dateString, "%Y-%m-%d", tz="UTC")
# }
loadIQN.convertDatetime <- function(datetimeString) {
as.POSIXct(datetimeString, "%Y-%m-%dT%H:%M:%S", tz="UTC")
}
## this function load the tsv file in to a dataframe with column names
loadIQN.load <- function() {
# Load the combined file
####================change this file name and directory=======================
ce <- "inferess-news-analytics-v1-WER-2016-06-02-2016-08-09-278868-records.tsv"
dataDir <- "~/Google Drive/Haas MFE 2016/Inferess Quantified News"
####==========================================================================
ceFiles <- paste(dataDir, ce, sep = '/')
for(ceFile in ceFiles) {
cat("File    : ", ceFile, "\n", sep = '')
}
cat("Loading    : ", ceFiles[1], "\n", sep = '')
nextData <- read.delim(ceFiles[1], encoding="UTF-8", header=FALSE, stringsAsFactors = FALSE)
if(length(ceFiles) > 1) {
for(ceFile in ceFiles[2:length(ceFiles)]) {
cat("Loading    : ", ceFile, "\n", sep = '')
nextData <- rbind(nextData, read.delim(ceFile, encoding="UTF-8", header=FALSE, stringsAsFactors = FALSE))
}
}
ceData <- nextData
recordCount <- nrow(ceData)
cat("Articles: ", recordCount, "\n", sep = '')
cat("\n")
# Rename some columns
names(ceData)[1] <- "md5"
names(ceData)[2] <- "display_date"
names(ceData)[3] <- "title"
names(ceData)[4] <- "sentiment"
names(ceData)[5] <- "neg_conf"
names(ceData)[6] <- "pos_conf"
names(ceData)[7] <- "neg_uni"
names(ceData)[8] <- "pos_uni"
names(ceData)[9] <- "tot_uni"
names(ceData)[10] <- "tickers"
names(ceData)[11] <- "isins"
names(ceData)[12] <- "topics"
# 2015-04-14T03:10:53.373Z
#ceData$display_date <- as.POSIXct(ceData$display_date, "%Y-%m-%dT%H:%M:%S", tz="UTC")
ceData$display_date <- loadIQN.convertDatetime(ceData$display_date)
ceData$neg_conf <- as.numeric(ceData$neg_conf)
ceData$pos_conf <- as.numeric(ceData$pos_conf)
ceData$neg_uni <- as.numeric(ceData$neg_uni)
ceData$pos_uni <- as.numeric(ceData$pos_uni)
ceData$tot_uni <- as.numeric(ceData$tot_uni)
ceData
}
## load the data into dataframe
iqnData <- loadIQN.load()
## more data cleansing
## let's take a look of the dataframe
View(iqnData, title = "IQN")
str(iqnData)
# Load IQN for WQ
# loadIQN.convertDate <- function(dateString) {
#   as.POSIXct(dateString, "%Y-%m-%d", tz="UTC")
# }
loadIQN.convertDatetime <- function(datetimeString) {
as.POSIXct(datetimeString, "%Y-%m-%dT%H:%M:%S", tz="UTC")
}
## this function load the tsv file in to a dataframe with column names
loadIQN.load <- function() {
# Load the combined file
####================change this file name and directory=======================
ce <- "inferess-news-analytics-v1-WER-2016-06-02-2016-08-09-278868-records.tsv"
dataDir <- "~/Google Drive/Haas MFE 2016/Inferess Quantified News"
####==========================================================================
ceFiles <- paste(dataDir, ce, sep = '/')
for(ceFile in ceFiles) {
cat("File    : ", ceFile, "\n", sep = '')
}
cat("Loading    : ", ceFiles[1], "\n", sep = '')
nextData <- read.delim(ceFiles[1], encoding="UTF-8", header=FALSE, stringsAsFactors = FALSE, fill = NA)
if(length(ceFiles) > 1) {
for(ceFile in ceFiles[2:length(ceFiles)]) {
cat("Loading    : ", ceFile, "\n", sep = '')
nextData <- rbind(nextData, read.delim(ceFile, encoding="UTF-8", header=FALSE, stringsAsFactors = FALSE))
}
}
ceData <- nextData
recordCount <- nrow(ceData)
cat("Articles: ", recordCount, "\n", sep = '')
cat("\n")
# Rename some columns
names(ceData)[1] <- "md5"
names(ceData)[2] <- "display_date"
names(ceData)[3] <- "title"
names(ceData)[4] <- "sentiment"
names(ceData)[5] <- "neg_conf"
names(ceData)[6] <- "pos_conf"
names(ceData)[7] <- "neg_uni"
names(ceData)[8] <- "pos_uni"
names(ceData)[9] <- "tot_uni"
names(ceData)[10] <- "tickers"
names(ceData)[11] <- "isins"
names(ceData)[12] <- "topics"
# 2015-04-14T03:10:53.373Z
#ceData$display_date <- as.POSIXct(ceData$display_date, "%Y-%m-%dT%H:%M:%S", tz="UTC")
ceData$display_date <- loadIQN.convertDatetime(ceData$display_date)
ceData$neg_conf <- as.numeric(ceData$neg_conf)
ceData$pos_conf <- as.numeric(ceData$pos_conf)
ceData$neg_uni <- as.numeric(ceData$neg_uni)
ceData$pos_uni <- as.numeric(ceData$pos_uni)
ceData$tot_uni <- as.numeric(ceData$tot_uni)
ceData
}
## load the data into dataframe
iqnData <- loadIQN.load()
## more data cleansing
## let's take a look of the dataframe
View(iqnData, title = "IQN")
# Load IQN for WQ
# loadIQN.convertDate <- function(dateString) {
#   as.POSIXct(dateString, "%Y-%m-%d", tz="UTC")
# }
loadIQN.convertDatetime <- function(datetimeString) {
as.POSIXct(datetimeString, "%Y-%m-%dT%H:%M:%S", tz="UTC")
}
## this function load the tsv file in to a dataframe with column names
loadIQN.load <- function() {
# Load the combined file
####================change this file name and directory=======================
ce <- "inferess-news-analytics-v1-WER-2016-06-02-2016-08-09-278868-records.tsv"
dataDir <- "~/Google Drive/Haas MFE 2016/Inferess Quantified News"
####==========================================================================
ceFiles <- paste(dataDir, ce, sep = '/')
for(ceFile in ceFiles) {
cat("File    : ", ceFile, "\n", sep = '')
}
cat("Loading    : ", ceFiles[1], "\n", sep = '')
nextData <- read.delim(ceFiles[1], encoding="UTF-8", header=FALSE, stringsAsFactors = FALSE, fill = TRUE)
if(length(ceFiles) > 1) {
for(ceFile in ceFiles[2:length(ceFiles)]) {
cat("Loading    : ", ceFile, "\n", sep = '')
nextData <- rbind(nextData, read.delim(ceFile, encoding="UTF-8", header=FALSE, stringsAsFactors = FALSE))
}
}
ceData <- nextData
recordCount <- nrow(ceData)
cat("Articles: ", recordCount, "\n", sep = '')
cat("\n")
# Rename some columns
names(ceData)[1] <- "md5"
names(ceData)[2] <- "display_date"
names(ceData)[3] <- "title"
names(ceData)[4] <- "sentiment"
names(ceData)[5] <- "neg_conf"
names(ceData)[6] <- "pos_conf"
names(ceData)[7] <- "neg_uni"
names(ceData)[8] <- "pos_uni"
names(ceData)[9] <- "tot_uni"
names(ceData)[10] <- "tickers"
names(ceData)[11] <- "isins"
names(ceData)[12] <- "topics"
# 2015-04-14T03:10:53.373Z
#ceData$display_date <- as.POSIXct(ceData$display_date, "%Y-%m-%dT%H:%M:%S", tz="UTC")
ceData$display_date <- loadIQN.convertDatetime(ceData$display_date)
ceData$neg_conf <- as.numeric(ceData$neg_conf)
ceData$pos_conf <- as.numeric(ceData$pos_conf)
ceData$neg_uni <- as.numeric(ceData$neg_uni)
ceData$pos_uni <- as.numeric(ceData$pos_uni)
ceData$tot_uni <- as.numeric(ceData$tot_uni)
ceData
}
## load the data into dataframe
iqnData <- loadIQN.load()
## more data cleansing
## let's take a look of the dataframe
View(iqnData, title = "IQN")
str(iqnData$sentiment)
# Load IQN for WQ
# loadIQN.convertDate <- function(dateString) {
#   as.POSIXct(dateString, "%Y-%m-%d", tz="UTC")
# }
loadIQN.convertDatetime <- function(datetimeString) {
as.POSIXct(datetimeString, "%Y-%m-%dT%H:%M:%S", tz="UTC")
}
## this function load the tsv file in to a dataframe with column names
loadIQN.load <- function() {
# Load the combined file
####================change this file name and directory=======================
ce <- "inferess-news-analytics-v1-WER-2016-06-02-2016-08-09-278868-records.tsv"
dataDir <- "~/Google Drive/Haas MFE 2016/Inferess Quantified News"
####==========================================================================
ceFiles <- paste(dataDir, ce, sep = '/')
for(ceFile in ceFiles) {
cat("File    : ", ceFile, "\n", sep = '')
}
cat("Loading    : ", ceFiles[1], "\n", sep = '')
nextData <- read.delim(ceFiles[1], encoding="UTF-8", header=FALSE, stringsAsFactors = FALSE, fill = TRUE, na.strings = "")
if(length(ceFiles) > 1) {
for(ceFile in ceFiles[2:length(ceFiles)]) {
cat("Loading    : ", ceFile, "\n", sep = '')
nextData <- rbind(nextData, read.delim(ceFile, encoding="UTF-8", header=FALSE, stringsAsFactors = FALSE))
}
}
ceData <- nextData
recordCount <- nrow(ceData)
cat("Articles: ", recordCount, "\n", sep = '')
cat("\n")
# Rename some columns
names(ceData)[1] <- "md5"
names(ceData)[2] <- "display_date"
names(ceData)[3] <- "title"
names(ceData)[4] <- "sentiment"
names(ceData)[5] <- "neg_conf"
names(ceData)[6] <- "pos_conf"
names(ceData)[7] <- "neg_uni"
names(ceData)[8] <- "pos_uni"
names(ceData)[9] <- "tot_uni"
names(ceData)[10] <- "tickers"
names(ceData)[11] <- "isins"
names(ceData)[12] <- "topics"
# 2015-04-14T03:10:53.373Z
#ceData$display_date <- as.POSIXct(ceData$display_date, "%Y-%m-%dT%H:%M:%S", tz="UTC")
ceData$display_date <- loadIQN.convertDatetime(ceData$display_date)
ceData$neg_conf <- as.numeric(ceData$neg_conf)
ceData$pos_conf <- as.numeric(ceData$pos_conf)
ceData$neg_uni <- as.numeric(ceData$neg_uni)
ceData$pos_uni <- as.numeric(ceData$pos_uni)
ceData$tot_uni <- as.numeric(ceData$tot_uni)
ceData
}
## load the data into dataframe
iqnData <- loadIQN.load()
## more data cleansing
## let's take a look of the dataframe
View(iqnData, title = "IQN")
str(iqnData)
library(tidyr)
library(dplyr)
install.packages('tidyr')
library(tidyr)
library(dplyr)
iqnData %>%
mutate(tickers = strsplit(tickers, ","), isins = strsplit(isins, ",")) %>%
unnest(tickers, isins)
unnest(tickers)
iqnData %>%
mutate(tickers = strsplit(tickers, ","), isins = strsplit(isins, ",")) %>%
unnest(tickers)
iqnData_1 <- iqnData %>%
mutate(tickers = strsplit(tickers, ","), isins = strsplit(isins, ",")) %>%
unnest(tickers)
View(iqnData_1)
View(iqnData_1)
iqnData <- loadIQN.load()
View(iqnData)
View(iqnData)
View(iqnData, title = "IQN")
iqnData <- loadIQN.load()
View(iqnData)
View(iqnData)
iqnData_1 <- iqnData %>%
mutate(tickers = strsplit(tickers, ",")) %>%
unnest(tickers)
View(iqnData_1)
iqnData_1 <- iqnData[!is.na(iqnData$tickers)] %>%
mutate(tickers = strsplit(tickers, ",")) %>%
unnest(tickers)
iqnData_1 <- iqnData[!is.na(iqnData$tickers),] %>%
mutate(tickers = strsplit(tickers, ",")) %>%
unnest(tickers)
library(tidyr)
library(dplyr)
# Load IQN for WQ
# loadIQN.convertDate <- function(dateString) {
#   as.POSIXct(dateString, "%Y-%m-%d", tz="UTC")
# }
loadIQN.convertDatetime <- function(datetimeString) {
as.POSIXct(datetimeString, "%Y-%m-%dT%H:%M:%S", tz="UTC")
}
## this function load the tsv file in to a dataframe with column names
loadIQN.load <- function() {
# Load the combined file
####================change this file name and directory=======================
ce <- "inferess-news-analytics-v1-WER-2016-06-02-2016-08-09-278868-records.tsv"
dataDir <- "~/Google Drive/Haas MFE 2016/Inferess Quantified News"
####==========================================================================
ceFiles <- paste(dataDir, ce, sep = '/')
for(ceFile in ceFiles) {
cat("File    : ", ceFile, "\n", sep = '')
}
cat("Loading    : ", ceFiles[1], "\n", sep = '')
## read the data one-by-one, fill empty slots with NAs
nextData <- read.delim(ceFiles[1], encoding="UTF-8", header=FALSE, stringsAsFactors = FALSE, fill = TRUE, na.strings = "")
if(length(ceFiles) > 1) {
for(ceFile in ceFiles[2:length(ceFiles)]) {
cat("Loading    : ", ceFile, "\n", sep = '')
nextData <- rbind(nextData, read.delim(ceFile, encoding="UTF-8", header=FALSE, stringsAsFactors = FALSE, fill = TRUE, na.strings = ""))
}
}
ceData <- nextData
recordCount <- nrow(ceData)
cat("Articles: ", recordCount, "\n", sep = '')
cat("\n")
# Rename some columns
names(ceData)[1] <- "md5"
names(ceData)[2] <- "display_date"
names(ceData)[3] <- "title"
names(ceData)[4] <- "sentiment"
names(ceData)[5] <- "neg_conf"
names(ceData)[6] <- "pos_conf"
names(ceData)[7] <- "neg_uni"
names(ceData)[8] <- "pos_uni"
names(ceData)[9] <- "tot_uni"
names(ceData)[10] <- "tickers"
names(ceData)[11] <- "isins"
names(ceData)[12] <- "topics"
# 2015-04-14T03:10:53.373Z
#ceData$display_date <- as.POSIXct(ceData$display_date, "%Y-%m-%dT%H:%M:%S", tz="UTC")
ceData$display_date <- loadIQN.convertDatetime(ceData$display_date)
ceData$neg_conf <- as.numeric(ceData$neg_conf)
ceData$pos_conf <- as.numeric(ceData$pos_conf)
ceData$neg_uni <- as.numeric(ceData$neg_uni)
ceData$pos_uni <- as.numeric(ceData$pos_uni)
ceData$tot_uni <- as.numeric(ceData$tot_uni)
ceData <- ceData[!is.na(iqnData$tickers),] %>%
mutate(tickers = strsplit(tickers, ",")) %>%
unnest(tickers)
ceData
}
## load the data into dataframe
iqnData <- loadIQN.load()
## more data cleansing
## let's take a look of the dataframe
View(iqnData, title = "IQN")
library(tidyr)
library(dplyr)
# Load IQN for WQ
# loadIQN.convertDate <- function(dateString) {
#   as.POSIXct(dateString, "%Y-%m-%d", tz="UTC")
# }
loadIQN.convertDatetime <- function(datetimeString) {
as.POSIXct(datetimeString, "%Y-%m-%dT%H:%M:%S", tz="UTC")
}
## this function load the tsv file in to a dataframe with column names
loadIQN.load <- function() {
# Load the combined file
####================change this file name and directory=======================
ce <- "inferess-news-analytics-v1-WER-2016-06-02-2016-08-09-278868-records.tsv"
dataDir <- "~/Google Drive/Haas MFE 2016/Inferess Quantified News"
####==========================================================================
ceFiles <- paste(dataDir, ce, sep = '/')
for(ceFile in ceFiles) {
cat("File    : ", ceFile, "\n", sep = '')
}
cat("Loading    : ", ceFiles[1], "\n", sep = '')
## read the data one-by-one, fill empty slots with NAs
nextData <- read.delim(ceFiles[1], encoding="UTF-8", header=FALSE, stringsAsFactors = FALSE, fill = TRUE, na.strings = "")
if(length(ceFiles) > 1) {
for(ceFile in ceFiles[2:length(ceFiles)]) {
cat("Loading    : ", ceFile, "\n", sep = '')
nextData <- rbind(nextData, read.delim(ceFile, encoding="UTF-8", header=FALSE, stringsAsFactors = FALSE, fill = TRUE, na.strings = ""))
}
}
ceData <- nextData
recordCount <- nrow(ceData)
cat("Articles: ", recordCount, "\n", sep = '')
cat("\n")
# Rename some columns
names(ceData)[1] <- "md5"
names(ceData)[2] <- "display_date"
names(ceData)[3] <- "title"
names(ceData)[4] <- "sentiment"
names(ceData)[5] <- "neg_conf"
names(ceData)[6] <- "pos_conf"
names(ceData)[7] <- "neg_uni"
names(ceData)[8] <- "pos_uni"
names(ceData)[9] <- "tot_uni"
names(ceData)[10] <- "tickers"
names(ceData)[11] <- "isins"
names(ceData)[12] <- "topics"
# 2015-04-14T03:10:53.373Z
#ceData$display_date <- as.POSIXct(ceData$display_date, "%Y-%m-%dT%H:%M:%S", tz="UTC")
ceData$display_date <- loadIQN.convertDatetime(ceData$display_date)
ceData$neg_conf <- as.numeric(ceData$neg_conf)
ceData$pos_conf <- as.numeric(ceData$pos_conf)
ceData$neg_uni <- as.numeric(ceData$neg_uni)
ceData$pos_uni <- as.numeric(ceData$pos_uni)
ceData$tot_uni <- as.numeric(ceData$tot_uni)
## rows without tickers are filtered out, and each row now corresponds to one ticker
# ceData <- ceData[!is.na(iqnData$tickers),] %>%
#    mutate(tickers = strsplit(tickers, ",")) %>%
#    unnest(tickers)
ceData
}
## load the data into dataframe
iqnData <- loadIQN.load()
## let's take a look of the dataframe
View(iqnData, title = "IQN")
library(jsonlite)
library(parallel) #mac version of parallel lapply
library(snow)     #windows version of parallel lapply
library(doParallel)#parallel on both
library(plyr)
library(dplyr)
library(urltools)
library(data.table)
library(jsonlite)
library(parallel) #mac version of parallel lapply
library(snow)     #windows version of parallel lapply
library(doParallel)#parallel on both
library(plyr)
library(dplyr)
library(urltools)
library(data.table)
##############################
json_file <- "/Users/tianyixia/Google Drive/ShareThis/Frank - R Code/part-000000000346.log"
pc_cores <- detectCores()
cl<-makeCluster( 16, type="SOCK") #  choose how many cores to use
json_data <- parLapply(cl, readLines(json_file), fromJSON, flatten = TRUE)
cond <- sapply(json_data, function(x) dim(x$companies)[2]>0)
json_data<- mclapply(readLines(input), fromJSON, flatten = TRUE)
json_data<- mclapply(readLines(json_file), fromJSON, flatten = TRUE)
cond <- sapply(json_data, function(x) dim(x$companies)[2]>0)
raw_json_dt <- do.call(rbindlist,list( l = parLapply(cl, json_data[cond], as.data.frame), fill = T))
raw_json_dt <- as.data.table(do.call(rbind.fill, parLapply(cl, json_data[cond], as.data.frame)))  # this step is gonna be very slow...
raw_json_dt <- do.call(rbindlist,list( l = mclappy(json_data[cond], as.data.frame), fill = T))
mclappy
raw_json_dt <- as.data.table(do.call(rbind.fill, mclappy(json_data[cond], as.data.frame)))  # this step is gonna be very slow...
raw_json_dt <- as.data.table(do.call(rbind.fill, lappy(cl, json_data[cond], as.data.frame)))  # this step is gonna be very slow...
library(parallel) #mac version of parallel lapply
raw_json_dt <- as.data.table(do.call(rbind.fill, mclappy(cl, json_data[cond], as.data.frame)))  # this step is gonna be very slow...
raw_json_dt <- do.call(rbindlist,list( l = mclappy(json_data[cond], as.data.frame), fill = T))
raw_json_dt <- do.call(rbindlist,list( l = mclapply(json_data[cond], as.data.frame), fill = T))
str)json_data
str(json_data)
str(json_data)
str(raw_json_dt)
library(jsonlite)
fromJSON(txt = '~/Downloads/test')
json_file <- "~/Downloads/test"
json_data<- mclapply(readLines(json_file), fromJSON, flatten = TRUE) #parse each line of the JSON and combine into a list
library(doParallel)#parallel on both
library(parallel) #mac version of parallel lapply
json_data<- mclapply(readLines(json_file), fromJSON, flatten = TRUE) #parse each line of the JSON and combine into a list
as.data.frame(json_data)
a = as.data.frame(json_data)
raw_json_dt <- do.call(rbindlist,list( l = mclapply(json_data[cond], as.data.frame), fill = T))
cond <- sapply(json_data, function(x) dim(x$companies)[2]>0)
raw_json_dt <- do.call(rbindlist,list( l = mclapply(json_data[cond], as.data.frame), fill = T))
raw_json_dt <- do.call(rbindlist,list( l = mclapply(json_data, as.data.frame), fill = T))
library(plyr)
library(dplyr)
library(urltools)
library(data.table)
raw_json_dt <- do.call(rbindlist,list( l = mclapply(json_data, as.data.frame), fill = T))
View(raw_json_dt)
View(raw_json_dt)
library(jsonlite)
library(parallel) #mac version of parallel lapply
library(snow)     #windows version of parallel lapply
library(doParallel)#parallel on both
library(plyr)
library(dplyr)
library(urltools)
library(data.table)
json_file <- "/Users/tianyixia/FranktheTank_git/ShareThis.txt/part-000000000000.log"
pc_cores <- detectCores()
json_data<- mclapply(readLines(json_file), fromJSON, flatten = TRUE) #parse each line of the JSON and combine into a list
cond <- sapply(json_data, function(x) dim(x$companies)[2]>0)
sum(cond) / length(json_data) # only 25% of the data are useful in this case
cleaning_json_dt <- raw_json_dt[!is.na(companies.ric),]
raw_json_dt <- do.call(rbindlist,list( l = mclapply(json_data[cond], as.data.frame), fill = T))
cleaning_json_dt <- raw_json_dt[!is.na(companies.ric),]
str(raw_json_dt)
cleaning_json_dt <- cleaning_json_dt[, c("companies.sentiment_score",
"companies.count",
"companies.sentiment",
"standardTimestamp",
"companies.ric",
"domain") :=
list(as.numeric(companies.sentiment_score),
as.numeric(companies.count),
as.factor(companies.sentiment),
# we are assuming the time is PDT
as.POSIXct(standardTimestamp, "%Y-%m-%dT%H:%M:%S", tz = "America/Los_Angeles"),
as.factor(companies.ric),
domain(url))
]
summary_json_domain <- cleaning_json_dt[, list(  ave_sentiment_score = mean( companies.sentiment_score, na.rm = T),
count = length(companies.sentiment_score) )
, by=domain] [ order(-count)]
summary_json_ticker <- cleaning_json_dt[, list(  ave_sentiment_score = mean( companies.sentiment_score, na.rm = T),
count = length(companies.sentiment_score) )
, by=companies.ric] [ order(-count)]
summary_json_ticker
summary_json_domain
str(raw_json_dt)
head(summary_json_ticker, 10)
summary_json_domain
head(summary_json_domain,1)
head(summary_json_domain,10)
head(summary_json_ticker, 10)
